{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the models obtain previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.set_context('paper', font_scale=1.4)\n",
    "sns.set(rc={\"figure.dpi\":300, 'savefig.dpi':300})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 100\n",
    "subselection = True\n",
    "mode = \"admission\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = pd.read_csv('data/outcomes_first_day{}.csv'.format('_subselection' if subselection else ''), index_col = 0)\n",
    "outcomes['Death'] = ~outcomes.Death.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"admission\":\n",
    "    results_weekend = 'results{}/weekends/'.format('_subselection' if subselection else '') # Train only on weekends but test on both\n",
    "    results_weekdays  = 'results{}/weekdays/'.format('_subselection' if subselection else '') # Train only on weekdays but test on both\n",
    "    periods = [\"Weekend\", \"Weekday\"]\n",
    "elif mode == \"gender\":\n",
    "    results_weekend = 'results{}/female/'.format('_subselection' if subselection else '')\n",
    "    results_weekdays  = 'results{}/male/'.format('_subselection' if subselection else '')\n",
    "    periods = [\"Female\", \"Male\"]\n",
    "# Random split\n",
    "results_random  = 'results{}/random/'.format('_subselection' if subselection else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "root = 'survival_'\n",
    "for period, results in zip(periods + [\"Random\"], [results_weekend, results_weekdays, results_random]):\n",
    "    predictions[period] = {}\n",
    "    print(period)\n",
    "    for file in sorted(os.listdir(results)):\n",
    "        if (root not in file) or ('.csv' not in file) or ('under' in file):\n",
    "            continue\n",
    "        \n",
    "        name = file[file.index(root)+len(root):file.rindex('.csv')]\n",
    "        predictions[period][name] = pd.read_csv(results + file, index_col=0)\n",
    "        print(file, ' -> ', name)\n",
    "\n",
    "# Select only if present in both\n",
    "intersection = predictions[periods[0]].keys() & predictions[periods[1]].keys()\n",
    "labels = {}\n",
    "for period in periods:\n",
    "    predictions[period] = {model: predictions[period][model] for model in intersection}\n",
    "    labels[period] = predictions[period][list(intersection)[0]].Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, brier_score_loss\n",
    "from sksurv.metrics import concordance_index_ipcw, integrated_brier_score, brier_score, cumulative_dynamic_auc\n",
    "from lifelines.fitters.kaplan_meier_fitter import KaplanMeierFitter\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaplman Meier estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [1, 7, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = labels[periods[0]] != 'External'\n",
    "kmf = KaplanMeierFitter(label = periods[0])\n",
    "kmf.fit(outcomes.LOS[test], outcomes.Death[test])\n",
    "kmf.plot()\n",
    "\n",
    "test = labels[periods[1]] != 'External'\n",
    "kmf = KaplanMeierFitter(label = periods[1])\n",
    "kmf.fit(outcomes.LOS[test], outcomes.Death[test])\n",
    "kmf.plot()\n",
    "\n",
    "plt.grid(alpha = 0.3)\n",
    "plt.xlabel('Time (in days)')\n",
    "plt.ylabel('Survival estimation')\n",
    "plt.xlim(0, 25)\n",
    "plt.ylim(0.7, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differencesin observed labels between training and testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate all metrics on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_calibration(e, t, survival, time):\n",
    "    # Create calibration curve\n",
    "    ## Bins the survival\n",
    "    bins = np.unique(np.percentile(survival, np.linspace(0, 100, 10)))\n",
    "    middle_bins = (bins[:-1] + bins[1:]) / 2\n",
    "    bins[0] -= 1e-8 # Ensure to be in the bin\n",
    "    split = np.digitize(survival, bins, right = True) - 1\n",
    "\n",
    "    ## Extimate death for each\n",
    "    proportion = {}\n",
    "    for i in sorted(np.unique(split)):\n",
    "        kmf = KaplanMeierFitter()\n",
    "        kmf.fit(t[split == i], e[split == i])\n",
    "        proportion[middle_bins[i]] = kmf.survival_function_at_times(time)\n",
    "\n",
    "    ## Extimate intercept and slope for the model\n",
    "    lr = LinearRegression().fit(np.array(list(proportion.keys())).reshape(-1, 1), np.array([proportion[k] for k in proportion]).reshape(-1, 1))\n",
    "    \n",
    "    return lr.intercept_, lr.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(e_train, t_train, e_test, t_test, risk, iterations = iters):\n",
    "    et_train = np.array([(e_train[i], t_train[i]) for i in range(len(e_train))],\n",
    "                     dtype = [('e', bool), ('t', float)])\n",
    "    \n",
    "    cis, rocs, brs = {t: [] for t in horizons}, {t: [] for t in horizons}, {t: [] for t in horizons}\n",
    "    total = iterations\n",
    "    for i in trange(iterations):\n",
    "        bootstrap = np.random.choice(np.arange(len(risk)), size = len(risk), replace = True) \n",
    "        bootstrap = [j for j in bootstrap if (t_test[j] < np.max(t_train)) or (e_test[j] == 0)]\n",
    "        et_test = np.array([(e_test[j], t_test[j]) for j in bootstrap],\n",
    "                            dtype = [('e', bool), ('t', float)])\n",
    "\n",
    "        risk_iteration = risk[bootstrap]\n",
    "        survival_iteration = 1 - risk_iteration\n",
    "\n",
    "        try:\n",
    "            b = brier_score(et_train, et_test, survival_iteration, horizons)[1]\n",
    "            # Concordance and ROC for each time\n",
    "            for j, time in enumerate(horizons):\n",
    "                brs[time].append(b[j])\n",
    "                cis[time].append(concordance_index_ipcw(et_train, et_test, risk_iteration[:, j], time)[0])\n",
    "                rocs[time].append(cumulative_dynamic_auc(et_train, et_test, risk_iteration[:, j], time)[0][0])\n",
    "        except Exception as e:\n",
    "            total -= 1\n",
    "            continue\n",
    "            # a = np.array([t_test[j] for j in bootstrap])\n",
    "            # plt.hist(t_train, bins = 100)\n",
    "            # plt.hist(a, bins = 100)\n",
    "            # plt.show()\n",
    "            # raise e\n",
    "    print(\"Effective iterations: \", total)\n",
    "    result = {}\n",
    "    for horizon in horizons:\n",
    "        result.update({\n",
    "          (\"TD Concordance Index\", 'Mean', horizon): np.mean(cis[horizon]),\n",
    "          (\"TD Concordance Index\", 'Std', horizon): np.std(cis[horizon]), \n",
    "          (\"Brier Score\", 'Mean', horizon): np.mean(brs[horizon]),\n",
    "          (\"Brier Score\", 'Std', horizon): np.std(brs[horizon]),\n",
    "          (\"ROC AUC\", 'Mean', horizon): np.mean(rocs[horizon]),\n",
    "          (\"ROC AUC\", 'Std', horizon): np.std(rocs[horizon]),\n",
    "        })\n",
    "\n",
    "    return pd.Series({r: result[r] for r in sorted(result)}), cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute and display performances per group of model\n",
    "models_group = {\"Baselines\": [\"deepsurv\"],\n",
    "                \"Sequential\": [\"lstm\"],\n",
    "                \"Time aware\": [\"gru\"],\n",
    "                \"Proposed\": [\"joint\"]}\n",
    "\n",
    "performances, rocs = {}, {}\n",
    "for source in labels:\n",
    "    performances[source], rocs[source] = {}, {}\n",
    "    for target in labels:\n",
    "        print('-' * 42)\n",
    "        print('{} -> {}'.format(source, target))\n",
    "\n",
    "        performances[source][target], rocs[source][target] = {}, {}\n",
    "        for group in models_group:\n",
    "            print('*' * 21)\n",
    "            print(group)\n",
    "            \n",
    "            for model in sorted(predictions[source].keys()):\n",
    "                if not(any([m in model for m in models_group[group]])):\n",
    "                    continue\n",
    "                np.random.seed(42)\n",
    "                preds = predictions[source][model]\n",
    "\n",
    "                print(model)\n",
    "\n",
    "                test = labels[target] == 'Internal' # Use the data that will be used for both   \n",
    "                test = test[test].index\n",
    "\n",
    "                train = labels[target] == 'Train' # Use Kaplan meier on the training data of the target\n",
    "                train = train[train].index\n",
    "\n",
    "                performances[source][target][model], rocs[source][target][model] = evaluate(outcomes.Death.loc[train].values, outcomes.Remaining.loc[train].values,\n",
    "                        outcomes.Death.loc[test].values, outcomes.Remaining.loc[test].values,\n",
    "                        preds.loc[test][[str(h) for h in horizons]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances['Random'], rocs['Random'] = {'Random': {}}, {'Random': {}}\n",
    "for model in sorted(predictions['Random'].keys()):\n",
    "    np.random.seed(42)\n",
    "    preds = predictions['Random'][model]\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    test = preds.Use != 'Train'\n",
    "    test = test[test].index\n",
    "\n",
    "    train = preds.Use == 'Train'\n",
    "    train = train[train].index\n",
    "\n",
    "    performances['Random']['Random'][model], rocs['Random']['Random'][model] = evaluate(outcomes.Death.loc[train].values, outcomes.Remaining.loc[train].values,\n",
    "        outcomes.Death.loc[test].values, outcomes.Remaining.loc[test].values,\n",
    "        preds.loc[test][[str(h) for h in horizons]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in performances:\n",
    "    for target in performances[source]:\n",
    "        print('-' * 42)\n",
    "        print('{} -> {}'.format(source, target))\n",
    "        if source == target:\n",
    "            print(pd.concat(performances[source][source], axis = 1))\n",
    "\n",
    "        else:\n",
    "            print(pd.concat(performances[source][target], axis = 1))\n",
    "            print(pd.concat(performances[source][target], axis = 1) - pd.concat(performances[source][source], axis = 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = \"TD Concordance Index\" #\"ROC AUC\"\"Brier Score\"\"TD Concordance Index\"\n",
    "\n",
    "for source in performances:\n",
    "    for target in performances[source]:\n",
    "        performances_display = {\n",
    "            r\"$\\bf{DeepJointFineTune}$\": performances[source][target]['joint_full_finetune_value+time+mask'][plot],\n",
    "            r\"$\\bf{DeepJointFeature}$\": performances[source][target]['joint_value+time+mask'][plot],\n",
    "            \"Feature\": performances[source][target]['lstm_value+time+mask'][plot],\n",
    "            \"GRU-D\": performances[source][target]['gru_d+mask'][plot],\n",
    "            r\"$\\bf{DeepJoint}$\": performances[source][target]['joint+value'][plot],\n",
    "            \"Resample\": performances[source][target]['lstm+resampled'][plot],\n",
    "            \"Ignore\": performances[source][target]['lstm_value'][plot],\n",
    "            \"Last\": performances[source][target]['deepsurv_last'][plot],\n",
    "        }\n",
    "\n",
    "        performances_display = pd.concat(performances_display, axis = 1)\n",
    "\n",
    "        #plt.rcParams.update({'font.size': 12})\n",
    "        fig, axes = plt.subplots(ncols = len(horizons), sharey = True, sharex = True, figsize=(12,3))\n",
    "        print(\"{} -> {}\".format(source, target))\n",
    "        for i, ax in zip(horizons, axes):\n",
    "            perf_metric_mean = performances_display.loc['Mean', i]\n",
    "            perf_metric_std = 1.96 * performances_display.loc['Std', i] / np.sqrt(iters)\n",
    "            for j, model in enumerate(performances_display.columns):\n",
    "                p = ax.plot((perf_metric_mean[model] + perf_metric_std[model], perf_metric_mean[model] - perf_metric_std[model]), (j, j), alpha = 0.5)\n",
    "                ax.scatter(perf_metric_mean[model], j, s = 100, label = model, marker = '|', color = p[-1].get_color(),linewidths = 2)\n",
    "                \n",
    "            ax.grid(alpha = 0.3)\n",
    "            ax.set_yticks(range(len(performances_display.columns)))\n",
    "            ax.set_yticklabels(performances_display.columns)\n",
    "            ax.set_title('Performances at {} days'.format(i))\n",
    "            ax.set_xlabel(plot)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(pd.DataFrame.from_dict({m: [\"{:.3f} ({:.3f})\".format(performances_display.loc['Mean'].loc[m].loc[i], 1.96 * performances_display.loc['Std'].loc[m].loc[i] / np.sqrt(iters)) for i in performances_display.loc['Mean'].columns] for m in performances_display.loc['Mean'].index}, columns = performances_display.columns, orient = 'index').T.loc[::-1].to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "    'joint+value': 'Deep Joint Model on labs',\n",
    "    'lstm_value+time+mask' : 'DeepSurv on labs, time and mask',\n",
    "}\n",
    "\n",
    "x, y = 'joint+value', 'lstm_value+time+mask'\n",
    "horizon = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Censored by {} day'.format(horizon): outcomes[(~outcomes.Death) & (outcomes.LOS < int(horizon))].index,\n",
    "    'Dead by {} day'.format(horizon): outcomes[outcomes.Death & (outcomes.LOS < int(horizon))].index,\n",
    "    'Alive at {} day'.format(horizon): outcomes[outcomes.LOS > int(horizon)].index,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in data:\n",
    "    plt.figure()\n",
    "    #sns.kdeplot(x=predictions['Random'][x][horizon].loc[data[cat]], y=predictions['Random'][y][horizon].loc[data[cat]], fill=True)\n",
    "    plt.scatter(predictions['Random'][x][horizon].loc[data[cat]], predictions['Random'][y][horizon].loc[data[cat]], alpha = 0.4)\n",
    "\n",
    "    plt.xlabel(names[x])\n",
    "    plt.ylabel(names[y])\n",
    "    plt.plot([0, 1], [0, 1], ls = '--', alpha = 0.3, c = 'k')\n",
    "    plt.title('Prediction Death at {} day - {}'.format(horizon, cat))\n",
    "    plt.grid(alpha = 0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between weekend and weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = \"TD Concordance Index\" #\"ROC AUC\"\"Brier Score\"\"TD Concordance Index\"\n",
    "\n",
    "performances_display = {\n",
    "    s :{\n",
    "        t: {\n",
    "            r\"$\\bf{DeepJointFineTune}$\": performances[s][t]['joint_full_finetune_value+time+mask'][plot],\n",
    "            r\"$\\bf{DeepJointFeature}$\": performances[s][t]['joint_value+time+mask'][plot],\n",
    "            \"Feature\": performances[s][t]['lstm_value+time+mask'][plot],\n",
    "            \"Last\": performances[s][t]['deepsurv_last'][plot],\n",
    "        }\n",
    "        for t in performances[s]\n",
    "    }\n",
    "    for s in periods\n",
    "}\n",
    "\n",
    "rocs_display = {\n",
    "    s :{\n",
    "        t: {\n",
    "            r\"$\\bf{DeepJointFineTune}$\": rocs[s][t]['joint_full_finetune_value+time+mask'],\n",
    "            r\"$\\bf{DeepJointFeature}$\": rocs[s][t]['joint_value+time+mask'],\n",
    "            \"Feature\": rocs[s][t]['lstm_value+time+mask'],\n",
    "            \"GRU-D\": rocs[s][t]['gru_d+mask'],\n",
    "            r\"$\\bf{DeepJoint}$\": rocs[s][t]['joint+value'],\n",
    "            \"Resample\": rocs[s][t]['lstm+resampled'],\n",
    "            \"Ignore\": rocs[s][t]['lstm_value'],\n",
    "            \"Last\": rocs[s][t]['deepsurv_last'],\n",
    "        }\n",
    "        for t in rocs[s]\n",
    "    }\n",
    "    for s in periods\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons= [1, 7, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot double barh to display performances\n",
    "### Create dfs of mean and std\n",
    "for time in performances_display:\n",
    "    opposite = periods[1] if time == periods[0] else periods[0]\n",
    "\n",
    "    transfer = \"{} -> {}\".format(opposite, time)\n",
    "    training = \"{} -> {}\".format(time, time)\n",
    "\n",
    "    fig, axes = plt.subplots(ncols = len(horizons), figsize = (14, 3))\n",
    "    \n",
    "    axes[0].set_ylabel(transfer)\n",
    "    fig.suptitle('Performances {}'.format(time))\n",
    "\n",
    "    for ax, i, m in zip(axes, horizons, ['o', 'D', 'x', '.']):\n",
    "        perf_metric_mean = pd.DataFrame({\n",
    "                    transfer : pd.concat(performances_display[opposite][time], axis = 1).loc['Mean', i],\n",
    "                    training : pd.concat(performances_display[time][time], axis = 1).loc['Mean', i]\n",
    "                })\n",
    "        perf_metric_std = 1.96 * pd.DataFrame({\n",
    "                    transfer : pd.concat(performances_display[opposite][time], axis = 1).loc['Std', i],\n",
    "                    training : pd.concat(performances_display[time][time], axis = 1).loc['Std', i]\n",
    "                }) / np.sqrt(iters)\n",
    "        colors = list(plt.rcParams['axes.prop_cycle'])\n",
    "        colors[3] = colors[7]\n",
    "        for model, c in zip(perf_metric_mean.index, colors[:len(perf_metric_mean)]):\n",
    "            ax.scatter(perf_metric_mean.loc[model][training], perf_metric_mean.loc[model][transfer], color = c['color'], marker = m, alpha = 0.5, s = 100)\n",
    "            ax.plot([perf_metric_mean.loc[model][training] - perf_metric_std.loc[model][training], perf_metric_mean.loc[model][training] + perf_metric_std.loc[model][training]], [perf_metric_mean.loc[model][transfer], perf_metric_mean.loc[model][transfer]], color = c['color'])\n",
    "            ax.plot([perf_metric_mean.loc[model][training], perf_metric_mean.loc[model][training]], [perf_metric_mean.loc[model][transfer] - perf_metric_std.loc[model][transfer], perf_metric_mean.loc[model][transfer] + perf_metric_std.loc[model][transfer]], color = c['color'])\n",
    "\n",
    "        ax.plot([0, 1], [0, 1], color = 'k', ls = ':', alpha = 0.5)\n",
    "        ax.set_xlabel(training)\n",
    "        ax.grid(alpha = 0.5)\n",
    "\n",
    "        means = perf_metric_mean.mean()\n",
    "        margin = 0.05 if time == 'Weekday' else 0.15\n",
    "        ax.set_xlim(means[training] - margin, means[training] + margin)\n",
    "        ax.set_ylim(means[training] - margin, means[training] + margin)\n",
    "    else:\n",
    "        # Display\n",
    "        ## Legend\n",
    "        plt.scatter([],[], alpha = 0., label = \"Models\")\n",
    "        for model, c in zip(perf_metric_mean.index, colors[:len(perf_metric_mean)]):\n",
    "            plt.scatter([],[], color = c['color'], label = model)\n",
    "        plt.scatter([],[], alpha = 0., label = \" \")\n",
    "        plt.scatter([],[], alpha = 0., label = \"Horizons\")\n",
    "        for i, m in zip(horizons, ['o', 'D', 'x', '.']):\n",
    "            plt.scatter([],[], marker = m, label = i, color = 'k')\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "        plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time in rocs_display:\n",
    "    opposite = periods[1] if time == periods[0] else periods[0]\n",
    "\n",
    "    transfer = \"{} -> {}\".format(opposite, time)\n",
    "    training = \"{} -> {}\".format(time, time)\n",
    "    print(transfer, training)\n",
    "    rocs_diff = {}\n",
    "    for model in rocs_display[opposite][time]:\n",
    "        rocs_diff[model] = {}\n",
    "        for t in rocs_display[opposite][time][model]:\n",
    "            diff = np.array(rocs_display[opposite][time][model][t]) - np.array(rocs_display[time][time][model][t])\n",
    "            rocs_diff[model][('Mean', t)] = np.mean(diff)\n",
    "            rocs_diff[model][('Std', t)] = 1.96 * np.std(diff) / np.sqrt(len(diff))\n",
    " \n",
    "    print(pd.DataFrame.from_dict({m: [\"{:.3f} ({:.3f})\".format(rocs_diff[m][('Mean', i)], rocs_diff[m][('Std', i)]) for i in horizons] for m in rocs_diff}, columns = horizons, orient = 'index').loc[::-1].to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display distance to diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time in performances_display:\n",
    "    opposite = periods[1] if time == periods[0] else periods[0]\n",
    "\n",
    "    transfer = \"{} -> {}\".format(opposite, time)\n",
    "    training = \"{} -> {}\".format(time, time)\n",
    "\n",
    "    perf_metric_diff = pd.concat(performances_display[time][time], axis = 1).loc['Mean'] - pd.concat(performances_display[opposite][time], axis = 1).loc['Mean']\n",
    "    perf_metric_diff.plot.bar()\n",
    "\n",
    "    print('Performance Model {} - Performance Model {} on {} patients'.format(time, opposite, time))\n",
    "    plt.xlabel('Evaluation Horizons')\n",
    "    plt.ylabel('Performance Difference')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), title = 'Model')\n",
    "    plt.grid(alpha = 0.3)\n",
    "    plt.ylim(-0.075, 0.075)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a54f3b3a447186e9a4a83057d2abe8df010acd7b8f131225203d307ef84eba48"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('Jupyter': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
